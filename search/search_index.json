{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"pytest-codingagents","text":"<p>Test-driven prompt engineering for GitHub Copilot.</p> <p>Everyone copies instruction files from blog posts, adds \"you are a senior engineer\" to agent configs, and includes skills found on Reddit. But does any of it work? Are your instructions making your agent better \u2014 or just longer?</p> <p>You don't know, because you're not testing it.</p> <p>pytest-codingagents gives you a complete test\u2192optimize\u2192test loop for GitHub Copilot configurations:</p> <ol> <li>Write a test \u2014 define what the agent should do</li> <li>Run it \u2014 see it fail (or pass)</li> <li>Optimize \u2014 call <code>optimize_instruction()</code> to get a concrete suggestion</li> <li>A/B confirm \u2014 use <code>ab_run</code> to prove the change actually helps</li> <li>Ship it \u2014 you now have evidence, not vibes</li> </ol> <p>Currently supports GitHub Copilot via copilot-sdk. More agents (Claude Code, etc.) coming soon.</p> <pre><code>from pytest_codingagents import CopilotAgent, optimize_instruction\nimport pytest\n\n\nasync def test_docstring_instruction_works(ab_run):\n    \"\"\"Prove the docstring instruction actually changes output, and get a fix if it doesn't.\"\"\"\n    baseline = CopilotAgent(instructions=\"Write Python code.\")\n    treatment = CopilotAgent(\n        instructions=\"Write Python code. Add Google-style docstrings to every function.\"\n    )\n\n    b, t = await ab_run(baseline, treatment, \"Create math.py with add(a, b) and subtract(a, b).\")\n\n    assert b.success and t.success\n\n    if '\"\"\"' not in t.file(\"math.py\"):\n        suggestion = await optimize_instruction(\n            treatment.instructions or \"\",\n            t,\n            \"Agent should add docstrings to every function.\",\n        )\n        pytest.fail(f\"Docstring instruction was ignored.\\n\\n{suggestion}\")\n\n    assert '\"\"\"' not in b.file(\"math.py\"), \"Baseline should not have docstrings\"\n</code></pre>"},{"location":"#install","title":"Install","text":"<pre><code>uv add pytest-codingagents\n</code></pre> <p>Authenticate via <code>GITHUB_TOKEN</code> env var (CI) or <code>gh auth status</code> (local).</p>"},{"location":"#what-you-can-test","title":"What You Can Test","text":"Capability What it proves Guide A/B comparison Config B actually produces different (and better) output than Config A A/B Testing Instruction optimization Turn a failing test into a ready-to-use instruction fix Optimize Instructions Instructions Your custom instructions change agent behavior \u2014 not just vibes Getting Started Skills That domain knowledge file is helping, not being ignored Skill Testing Models Which model works best for your use case and budget Model Comparison Custom Agents Your custom agent configurations actually work as intended Getting Started MCP Servers The agent discovers and uses your custom tools MCP Server Testing CLI Tools The agent operates command-line interfaces correctly CLI Tool Testing Tool Control Allowlists and blocklists restrict tool usage Tool Control"},{"location":"#ai-analysis","title":"AI Analysis","text":"<p>See it in action: Basic Report \u00b7 Model Comparison \u00b7 Instruction Testing</p> <p>Every test run produces an HTML report with AI-powered insights:</p> <ul> <li>Diagnoses failures \u2014 root cause analysis with suggested fixes</li> <li>Compares models \u2014 leaderboards ranked by pass rate and cost</li> <li>Evaluates instructions \u2014 which instructions produce better results</li> <li>Recommends improvements \u2014 actionable changes to tools, instructions, and skills</li> </ul> <pre><code>uv run pytest tests/ --aitest-html=report.html --aitest-summary-model=azure/gpt-5.2-chat\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Full docs at sbroenne.github.io/pytest-codingagents \u2014 API reference, how-to guides, and demo reports.</p> <ul> <li>Getting Started \u2014 Install and write your first test</li> <li>How-To Guides \u2014 A/B testing, instruction optimization, skills, MCP, and more</li> <li>Demo Reports \u2014 See real HTML reports with AI analysis</li> <li>API Reference \u2014 Full API documentation</li> </ul>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#development-setup","title":"Development Setup","text":"<pre><code>git clone https://github.com/sbroenne/pytest-codingagents.git\ncd pytest-codingagents\nuv sync --all-extras\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Unit tests (fast, no Copilot needed)\nuv run pytest tests/unit/ -v\n\n# Integration tests (requires Copilot CLI + auth)\nuv run pytest tests/ -v -m copilot\n\n# Per-file reports (generates one HTML report per test file)\nuv run python scripts/run_all.py\n</code></pre>"},{"location":"contributing/#code-quality","title":"Code Quality","text":"<pre><code># Lint\nuv run ruff check src tests\n\n# Format\nuv run ruff format src tests\n\n# Type check\nuv run pyright src\n</code></pre> <p>Pre-commit hooks run automatically on <code>git commit</code>.</p>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>src/pytest_codingagents/\n\u251c\u2500\u2500 __init__.py              # Public API exports\n\u251c\u2500\u2500 plugin.py                # pytest plugin entry point + analysis prompt hook\n\u251c\u2500\u2500 prompts/\n\u2502   \u2514\u2500\u2500 coding_agent_analysis.md  # AI analysis prompt template\n\u2514\u2500\u2500 copilot/\n    \u251c\u2500\u2500 __init__.py          # Copilot subpackage exports\n    \u251c\u2500\u2500 agent.py             # CopilotAgent dataclass\n    \u251c\u2500\u2500 result.py            # CopilotResult, Turn, ToolCall\n    \u251c\u2500\u2500 events.py            # EventMapper (SDK event \u2192 result)\n    \u251c\u2500\u2500 runner.py            # run_copilot() execution engine\n    \u2514\u2500\u2500 fixtures.py          # copilot_run fixture + aitest bridge\n</code></pre>"},{"location":"demo/","title":"Demo Reports","text":"<p>Live HTML reports generated by pytest-codingagents with pytest-aitest integration. These reports include AI-powered analysis, tool call diagrams, and cost breakdowns.</p>"},{"location":"demo/#reports","title":"Reports","text":"Report Description Basic Report Core file operations \u2014 create modules, refactor code Model Comparison Same tasks across different models (GPT-5.2 vs Claude Opus 4.5) Instruction Testing How different instructions affect agent behavior"},{"location":"demo/#how-these-are-generated","title":"How These Are Generated","text":"<p>Each report is produced by running integration tests with pytest-aitest:</p> <pre><code>uv run pytest tests/test_basic.py \\\n    --aitest-html=docs/demo/basic-report.html \\\n    --aitest-summary-model=azure/gpt-5.2-chat\n</code></pre> <p>Or use the per-file runner to generate all reports at once:</p> <pre><code>uv run python scripts/run_all.py\n</code></pre> <p>See pytest-aitest Integration for setup details.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code>uv add pytest-codingagents\n</code></pre>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>GitHub Copilot access (the SDK uses the Copilot CLI bundled with the package)</li> <li>Authentication: either <code>gh</code> CLI login or a <code>GITHUB_TOKEN</code> environment variable</li> </ul>"},{"location":"getting-started/#your-first-test","title":"Your First Test","text":"<pre><code>from pytest_codingagents import CopilotAgent\n\n\nasync def test_hello_world(copilot_run, tmp_path):\n    agent = CopilotAgent(\n        name=\"hello-test\",\n        instructions=\"Create files as requested.\",\n        working_directory=str(tmp_path),\n    )\n    result = await copilot_run(agent, \"Create hello.py that prints 'hello world'\")\n\n    assert result.success\n    assert (tmp_path / \"hello.py\").exists()\n</code></pre>"},{"location":"getting-started/#how-it-works","title":"How It Works","text":"<ol> <li>You define a <code>CopilotAgent</code> \u2014 model, instructions, working directory</li> <li>You write a prompt \u2014 what the agent should do</li> <li>The SDK runs it \u2014 against the real Copilot CLI, no mocks</li> <li>You assert on <code>CopilotResult</code> \u2014 success, tool calls, file changes, tokens</li> </ol>"},{"location":"getting-started/#whats-next","title":"What's Next","text":"<ul> <li>Model Comparison \u2014 Compare different models</li> <li>Instruction Testing \u2014 Test different instructions</li> </ul>"},{"location":"getting-started/instruction-testing/","title":"Instruction Testing","text":"<p>A/B test different instruction configs against the same task and assert the outputs actually differ.</p>"},{"location":"getting-started/instruction-testing/#direct-comparison","title":"Direct Comparison","text":"<p>Run two configs against the same task and assert observable differences:</p> <pre><code>from pytest_codingagents import CopilotAgent\n\n\nasync def test_documentation_instructions_produce_docstrings(copilot_run, tmp_path):\n    \"\"\"Do documentation instructions actually change the output?\"\"\"\n    concise = CopilotAgent(\n        name=\"concise\",\n        instructions=\"Write minimal Python. NO docstrings. NO comments. Pure logic only.\",\n        working_directory=str(tmp_path / \"concise\"),\n    )\n    verbose = CopilotAgent(\n        name=\"verbose\",\n        instructions='Write fully documented Python. EVERY function MUST have a docstring: \"\"\"What it does.\"\"\"',\n        working_directory=str(tmp_path / \"verbose\"),\n    )\n    (tmp_path / \"concise\").mkdir()\n    (tmp_path / \"verbose\").mkdir()\n\n    task = \"Create calculator.py with add(a, b), subtract(a, b), multiply(a, b).\"\n    result_a = await copilot_run(concise, task)\n    result_b = await copilot_run(verbose, task)\n\n    assert result_a.success and result_b.success\n\n    content_concise = (tmp_path / \"concise\" / \"calculator.py\").read_text()\n    content_verbose = (tmp_path / \"verbose\" / \"calculator.py\").read_text()\n\n    # Assert the instruction actually changed the output\n    assert '\"\"\"' not in content_concise, \"Concise instructions should suppress docstrings\"\n    assert '\"\"\"' in content_verbose, \"Verbose instructions should produce docstrings\"\n</code></pre>"},{"location":"getting-started/instruction-testing/#parametrized-style-comparison","title":"Parametrized Style Comparison","text":"<p>Run many instruction variants in a single test suite and let the AI report rank them:</p> <pre><code>import pytest\nfrom pytest_codingagents import CopilotAgent\n\nINSTRUCTIONS = {\n    \"concise\": \"Write minimal, clean code. No comments unless complex.\",\n    \"verbose\": \"Write well-documented code with docstrings and inline comments.\",\n    \"tdd\": \"Always write tests first, then implement the solution.\",\n}\n\n\n@pytest.mark.parametrize(\"style,instructions\", INSTRUCTIONS.items())\nasync def test_coding_style(copilot_run, tmp_path, style, instructions):\n    agent = CopilotAgent(\n        name=f\"style-{style}\",\n        instructions=instructions,\n        working_directory=str(tmp_path),\n    )\n    result = await copilot_run(agent, \"Create a calculator module with add, subtract, multiply, divide\")\n    assert result.success\n    assert (tmp_path / \"calculator.py\").exists()\n</code></pre> <p>The AI analysis report will rank instruction styles by pass rate and highlight behavioral differences.</p>"},{"location":"getting-started/instruction-testing/#what-to-assert","title":"What To Assert","text":"<ul> <li>Presence of specific constructs \u2014 <code>assert '\"\"\"' in content</code> to verify docstrings</li> <li>Absence of forbidden constructs \u2014 <code>assert \"print(\" not in content</code></li> <li>Library choice \u2014 <code>assert \"fastapi\" in code.lower()</code></li> <li>Tool usage \u2014 <code>assert result.tool_was_called(\"run_in_terminal\")</code></li> </ul>"},{"location":"getting-started/model-comparison/","title":"Model Comparison","text":"<p>Compare how different models perform the same task \u2014 which is more reliable, more efficient, or better at following instructions.</p>"},{"location":"getting-started/model-comparison/#example","title":"Example","text":"<pre><code>import pytest\nfrom pytest_codingagents import CopilotAgent\n\nMODELS = [\"claude-opus-4.5\", \"gpt-5.2\"]\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\nasync def test_fibonacci(copilot_run, tmp_path, model):\n    agent = CopilotAgent(\n        name=f\"model-{model}\",\n        model=model,\n        instructions=\"Write clean Python code.\",\n        working_directory=str(tmp_path),\n    )\n    result = await copilot_run(agent, \"Create fibonacci.py with a fibonacci function\")\n\n    assert result.success\n    assert (tmp_path / \"fibonacci.py\").exists()\n</code></pre> <p>The AI analysis report will produce a leaderboard ranking models by pass rate, token efficiency, and turn count.</p>"},{"location":"getting-started/model-comparison/#what-to-compare","title":"What To Compare","text":"<ul> <li>Consistency \u2014 Run the same test multiple times. Which model varies least across runs?</li> <li>Turn count \u2014 <code>len(result.turns)</code> \u2014 fewer turns often means better instruction-following.</li> <li>Token usage \u2014 <code>result.total_tokens</code> \u2014 efficiency matters at scale.</li> <li>Tool call patterns \u2014 Does one model over-call tools or skip necessary ones?</li> </ul>"},{"location":"how-to/","title":"How-To Guides","text":"<p>Practical guides for common tasks.</p> <ul> <li>A/B Testing \u2014 Prove that your config changes actually make a difference</li> <li>Optimize Instructions \u2014 Use AI to turn test failures into actionable instruction improvements</li> <li>Assertions \u2014 File helpers and semantic assertions with <code>llm_assert</code></li> <li>Load from Copilot Config \u2014 Build a <code>CopilotAgent</code> from your real <code>.github/</code> config files</li> <li>Skill Testing \u2014 Measure the impact of domain knowledge</li> <li>MCP Server Testing \u2014 Test that the agent uses your custom tools</li> <li>CLI Tool Testing \u2014 Verify the agent operates CLI tools correctly</li> <li>Tool Control \u2014 Restrict tools with allowlists and blocklists</li> <li>pytest-aitest Integration \u2014 HTML reports with AI analysis</li> <li>CI/CD Integration \u2014 Run tests in GitHub Actions</li> </ul>"},{"location":"how-to/ab-testing/","title":"A/B Testing Agent Configs","text":"<p>The core use case of pytest-codingagents is A/B testing: run the same task with two different agent configurations and prove that one produces measurably better output than the other.</p> <p>This stops cargo cult configuration \u2014 copying instructions and skills from blog posts without knowing if they work.</p>"},{"location":"how-to/ab-testing/#the-ab_run-fixture","title":"The <code>ab_run</code> Fixture","text":"<p>The <code>ab_run</code> fixture is the fastest way to write an A/B test. It handles directory isolation, sequential execution, and aitest reporting automatically:</p> <pre><code>from pytest_codingagents import CopilotAgent\n\n\nasync def test_docstring_instruction(ab_run):\n    baseline = CopilotAgent(instructions=\"Write Python code.\")\n    treatment = CopilotAgent(\n        instructions=\"Write Python code. Add Google-style docstrings to every function.\"\n    )\n\n    b, t = await ab_run(baseline, treatment, \"Create math.py with add(a, b) and subtract(a, b).\")\n\n    assert b.success and t.success\n    assert '\"\"\"' not in b.file(\"math.py\"), \"Baseline should not have docstrings\"\n    assert '\"\"\"' in t.file(\"math.py\"), \"Treatment: docstring instruction was ignored\"\n</code></pre> <p><code>ab_run</code> automatically creates <code>baseline/</code> and <code>treatment/</code> subdirectories under <code>tmp_path</code>, overrides <code>working_directory</code> on each agent (so they never share a workspace), and runs them sequentially.</p>"},{"location":"how-to/ab-testing/#the-manual-pattern","title":"The Manual Pattern","text":"<p>For full control \u2014 custom paths, conditional logic, more than two configs \u2014 use <code>copilot_run</code> directly:</p> <pre><code>from pytest_codingagents import CopilotAgent\n\n\nasync def test_config_a_vs_config_b(copilot_run, tmp_path):\n    config_a = CopilotAgent(\n        name=\"baseline\",\n        instructions=\"...\",          # Config A\n        working_directory=str(tmp_path / \"a\"),\n    )\n    config_b = CopilotAgent(\n        name=\"treatment\",\n        instructions=\"...\",          # Config B \u2014 the change you're testing\n        working_directory=str(tmp_path / \"b\"),\n    )\n    (tmp_path / \"a\").mkdir()\n    (tmp_path / \"b\").mkdir()\n\n    task = \"Create calculator.py with add(a, b) and subtract(a, b).\"\n    result_a = await copilot_run(config_a, task)\n    result_b = await copilot_run(config_b, task)\n\n    assert result_a.success and result_b.success\n\n    # Assert the configs produced DIFFERENT, OBSERVABLE outputs\n    content_a = (tmp_path / \"a\" / \"calculator.py\").read_text()\n    content_b = (tmp_path / \"b\" / \"calculator.py\").read_text()\n    assert '\"\"\"' not in content_a   # baseline: no docstrings\n    assert '\"\"\"' in content_b       # treatment: docstrings present\n</code></pre> <p>The key rule: assert something that is present in Config B because of the change and absent (or different) in Config A.</p>"},{"location":"how-to/ab-testing/#testing-instructions","title":"Testing Instructions","text":"<p>Does adding a documentation mandate actually change the code written?</p> <pre><code>async def test_framework_instruction_steers_choice(copilot_run, tmp_path):\n    baseline = CopilotAgent(\n        name=\"generic\",\n        instructions=\"You are a Python developer.\",\n        working_directory=str(tmp_path / \"a\"),\n    )\n    with_fastapi = CopilotAgent(\n        name=\"fastapi\",\n        instructions=\"You are a Python developer. ALWAYS use FastAPI for web APIs.\",\n        working_directory=str(tmp_path / \"b\"),\n    )\n    (tmp_path / \"a\").mkdir()\n    (tmp_path / \"b\").mkdir()\n\n    task = 'Create a web API with a GET /health endpoint returning {\"status\": \"ok\"}.'\n    result_a = await copilot_run(baseline, task)\n    result_b = await copilot_run(with_fastapi, task)\n\n    assert result_a.success and result_b.success\n    code_b = \"\\n\".join(f.read_text() for f in (tmp_path / \"b\").rglob(\"*.py\"))\n    assert \"fastapi\" in code_b.lower(), \"FastAPI instruction was ignored\"\n</code></pre>"},{"location":"how-to/ab-testing/#testing-skills","title":"Testing Skills","text":"<p>Does adding a skill file actually change what the agent produces compared to the same task without it?</p> <pre><code>async def test_exports_skill_adds_all_declaration(copilot_run, tmp_path):\n    skill_dir = tmp_path / \"skills\"\n    skill_dir.mkdir()\n    (skill_dir / \"module-exports.md\").write_text(\n        \"# Module Export Standards\\n\\n\"\n        \"Every Python module MUST declare its public API using __all__.\\n\"\n        '__all__ = [\"FunctionName\"]\\n\\n'\n        \"Modules without __all__ are considered incomplete.\\n\"\n    )\n\n    task = \"Create math_utils.py with add(a, b) and subtract(a, b).\"\n\n    baseline_dir = tmp_path / \"baseline\"\n    baseline_dir.mkdir()\n    baseline = CopilotAgent(\n        name=\"baseline\",\n        instructions=\"Write a Python module.\",\n        working_directory=str(baseline_dir),\n    )\n\n    treatment_dir = tmp_path / \"treatment\"\n    treatment_dir.mkdir()\n    treatment = CopilotAgent(\n        name=\"treatment\",\n        instructions=\"Write a Python module. Apply all module export standards from your skills.\",\n        working_directory=str(treatment_dir),\n        skill_directories=[str(skill_dir)],\n    )\n\n    result_a = await copilot_run(baseline, task)\n    result_b = await copilot_run(treatment, task)\n\n    assert result_a.success and result_b.success\n\n    content_a = (baseline_dir / \"math_utils.py\").read_text()\n    content_b = (treatment_dir / \"math_utils.py\").read_text()\n\n    assert \"__all__\" not in content_a, \"Baseline should not have __all__ (LLM default)\"\n    assert \"__all__\" in content_b, \"Skill should have added __all__ declaration\"\n</code></pre>"},{"location":"how-to/ab-testing/#testing-models","title":"Testing Models","text":"<p>Which model follows instructions more reliably?</p> <pre><code>import pytest\nfrom pytest_codingagents import CopilotAgent\n\nMODELS = [\"claude-opus-4.5\", \"gpt-5.2\"]\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\nasync def test_model_follows_defensive_instructions(copilot_run, tmp_path, model):\n    agent = CopilotAgent(\n        name=f\"model-{model}\",\n        model=model,\n        instructions=(\n            \"Always write defensive Python. \"\n            \"All I/O operations MUST use try/except. Never let exceptions propagate uncaught.\"\n        ),\n        working_directory=str(tmp_path),\n    )\n    result = await copilot_run(\n        agent, \"Create file_reader.py with read_json(path) that returns parsed JSON.\"\n    )\n    assert result.success\n    content = (tmp_path / \"file_reader.py\").read_text()\n    assert \"try\" in content and \"except\" in content, (\n        f\"Model {model} ignored defensive coding instructions\"\n    )\n</code></pre> <p>The AI analysis report will produce a leaderboard showing which model followed instructions most reliably.</p>"},{"location":"how-to/ab-testing/#choosing-good-assertions","title":"Choosing Good Assertions","text":"<p>The hardest part of A/B testing is finding an observable signal that reliably differs between configs.</p> What you're testing Good signal Fragile signal Docstring instructions <code>'\"\"\"' in content</code> / <code>'\"\"\"' not in content</code> Token count Framework choice <code>\"fastapi\" in code.lower()</code> File count Skills (module exports) <code>\"__all__\" in content</code> Code length Skills (<code>__version__</code>) <code>\"__version__\" in content</code> Pass/fail alone Error handling <code>\"try\" in content and \"except\" in content</code> <code>result.success</code> Tool restrictions <code>not result.tool_was_called(\"run_in_terminal\")</code> Turn count <p>Avoid asserting only <code>result.success</code> \u2014 both configs will usually succeed. The point is to prove they produce different output.</p>"},{"location":"how-to/ab-testing/#reading-the-ai-report","title":"Reading the AI Report","text":"<p>After running A/B tests, generate the HTML report:</p> <pre><code>uv run pytest tests/ -m copilot --aitest-html=report.html --aitest-summary-model=azure/gpt-5.2-chat\n</code></pre> <p>The report will:</p> <ul> <li>Rank configs by pass rate in a leaderboard</li> <li>Highlight differences in tool usage, turn count, and token consumption</li> <li>Diagnose failures \u2014 if Config B fails and Config A passes, the AI explains why</li> <li>Recommend improvements \u2014 actionable changes to instructions, skills, or model choice</li> </ul>"},{"location":"how-to/aitest-integration/","title":"pytest-aitest Integration","text":"<p>HTML reports with AI-powered analysis are included automatically \u2014 pytest-aitest is a core dependency.</p> <p>See example reports: Basic Report \u00b7 Model Comparison \u00b7 Instruction Testing</p>"},{"location":"how-to/aitest-integration/#how-it-works","title":"How It Works","text":"<p>When tests run, <code>CopilotResult</code> automatically bridges to <code>AgentResult</code>, enabling:</p> <ul> <li>HTML reports with test results, tool call details, and Mermaid sequence diagrams</li> <li>AI analysis with failure root causes and improvement suggestions tailored for coding agents</li> <li>Agent leaderboards when comparing models or instructions</li> <li>Dynamic pricing \u2014 model costs pulled live from litellm for accurate cost analysis</li> </ul>"},{"location":"how-to/aitest-integration/#usage","title":"Usage","text":"<p>Use pytest-aitest's standard CLI options:</p> <pre><code>uv run pytest tests/ --aitest-html=report.html --aitest-summary-model=azure/gpt-5-mini\n</code></pre> <p>Or configure in <code>pyproject.toml</code>:</p> <pre><code>[tool.pytest.ini_options]\naddopts = \"\"\"\n    --aitest-html=aitest-reports/report.html\n    --aitest-summary-model=azure/gpt-5.2-chat\n\"\"\"\n</code></pre> <p>No code changes needed \u2014 the integration is automatic via the plugin system.</p>"},{"location":"how-to/aitest-integration/#analysis-prompt-hook","title":"Analysis Prompt Hook","text":"<p>The plugin implements the <code>pytest_aitest_analysis_prompt</code> hook to inject Copilot-specific context into AI analysis:</p> <ul> <li>Coding-agent framing \u2014 the AI analyzer understands it's evaluating models, instructions, and tools (not MCP servers)</li> <li>Dynamic pricing table \u2014 model pricing data is pulled live from litellm's <code>model_cost</code> database, so cost analysis stays current without manual updates</li> </ul> <p>This happens automatically \u2014 no configuration needed.</p>"},{"location":"how-to/assertions/","title":"Assertions","text":"<p>Two complementary assertion styles work together in pytest-codingagents:</p> <ul> <li>File helpers \u2014 read files the agent created/modified directly from <code>CopilotResult</code></li> <li>Semantic assertions \u2014 use the <code>llm_assert</code> fixture from pytest-aitest</li> </ul>"},{"location":"how-to/assertions/#file-helpers","title":"File helpers","text":"<p>The agent runs in <code>working_directory</code>. File helpers let you inspect results without threading <code>tmp_path</code> through every test.</p> <pre><code># Read a file the agent created\ncontent = result.file(\"main.py\")\n\n# Check existence\nassert result.file_exists(\"main.py\")\n\n# Find files by glob pattern (recursive)\npy_files = result.files_matching(\"**/*.py\")\nassert py_files, \"No Python files were created\"\n\ntest_files = result.files_matching(\"test_*.py\")\nassert test_files, \"No test file was created\"\n</code></pre> <p>All paths are relative to <code>agent.working_directory</code>. <code>file()</code> raises <code>FileNotFoundError</code> if the file does not exist \u2014 which is itself a useful assertion when you expect the agent to have created it.</p>"},{"location":"how-to/assertions/#semantic-assertions-with-llm_assert","title":"Semantic assertions with <code>llm_assert</code>","text":"<p><code>llm_assert</code> is a pytest fixture provided by pytest-aitest \u2014 it's automatically available in every test (no import needed). It evaluates text against a plain-English criterion using a cheap judge LLM.</p> <pre><code>async def test_calculator(copilot_run, llm_assert, tmp_path):\n    agent = CopilotAgent(\n        instructions=\"Write fully documented Python with Google-style docstrings.\",\n        working_directory=str(tmp_path),\n    )\n    result = await copilot_run(agent, \"Create calculator.py with add and subtract.\")\n    assert result.success\n    assert llm_assert(\n        result.file(\"calculator.py\"),\n        \"has Google-style docstrings with Args: and Returns: sections\",\n    )\n</code></pre> <p>See the pytest-aitest docs for full <code>llm_assert</code> and <code>llm_score</code> documentation.</p>"},{"location":"how-to/assertions/#combining-both-in-ab-tests","title":"Combining both in A/B tests","text":"<pre><code>@pytest.mark.copilot\nasync def test_docstring_instruction_adds_docs(copilot_run, llm_assert, tmp_path):\n    baseline_dir = tmp_path / \"baseline\"\n    baseline_dir.mkdir()\n    treatment_dir = tmp_path / \"treatment\"\n    treatment_dir.mkdir()\n\n    baseline = CopilotAgent(working_directory=str(baseline_dir))\n    treatment = CopilotAgent(\n        working_directory=str(treatment_dir),\n        instructions=\"Every function MUST have a Google-style docstring.\",\n    )\n\n    task = \"Create calculator.py with add(a, b) and subtract(a, b).\"\n    baseline_result = await copilot_run(baseline, task)\n    treatment_result = await copilot_run(treatment, task)\n\n    assert treatment_result.success\n    assert llm_assert(\n        treatment_result.file(\"calculator.py\"),\n        \"has Google-style docstrings with Args: and Returns: sections\",\n    ), \"Treatment did not produce docstrings despite instruction\"\n</code></pre>"},{"location":"how-to/assertions/#see-also","title":"See also","text":"<ul> <li>A/B Testing Guide</li> <li>Load from Copilot Config</li> </ul>"},{"location":"how-to/ci-cd/","title":"CI/CD Integration","text":"<p>Run integration tests in GitHub Actions.</p>"},{"location":"how-to/ci-cd/#manual-trigger","title":"Manual Trigger","text":"<p>The repository includes a <code>workflow_dispatch</code> workflow for running integration tests on demand.</p>"},{"location":"how-to/ci-cd/#prerequisites","title":"Prerequisites","text":"<ol> <li>Create a GitHub environment called <code>integration</code> in your repo settings</li> <li>Add a secret <code>COPILOT_GITHUB_TOKEN</code> with a GitHub token that has Copilot access</li> </ol>"},{"location":"how-to/ci-cd/#running","title":"Running","text":"<p>Go to Actions \u2192 Integration Tests \u2192 Run workflow.</p> <p>Optionally provide a <code>-k</code> filter to run specific tests (e.g., <code>test_basic</code>).</p>"},{"location":"how-to/ci-cd/#unit-tests-in-ci","title":"Unit Tests in CI","text":"<p>Unit tests run automatically on every push and PR via the <code>ci.yml</code> workflow. These don't require Copilot access \u2014 they test pure logic with no SDK calls.</p>"},{"location":"how-to/ci-cd/#token-management","title":"Token Management","text":"<p>Integration tests use real Copilot API calls:</p> <ul> <li>Keep tests focused \u2014 one prompt per test</li> <li>Use <code>max_turns</code> \u2014 limit conversation turns to control cost</li> <li>Use <code>timeout_s</code> \u2014 set reasonable timeouts to avoid runaway tests</li> <li>Filter with <code>-k</code> \u2014 run only the tests you need</li> </ul>"},{"location":"how-to/cli-tools/","title":"CLI Tool Testing","text":"<p>Test that GitHub Copilot can operate command-line tools correctly.</p>"},{"location":"how-to/cli-tools/#basic-usage","title":"Basic Usage","text":"<p>Give the agent a task that requires CLI tools and verify the outcome:</p> <pre><code>from pytest_codingagents import CopilotAgent\n\n\nasync def test_git_init(copilot_run, tmp_path):\n    agent = CopilotAgent(\n        instructions=\"Use git commands as requested.\",\n        working_directory=str(tmp_path),\n    )\n    result = await copilot_run(\n        agent,\n        \"Initialize a git repo, create a .gitignore for Python, and make an initial commit.\",\n    )\n    assert result.success\n    assert (tmp_path / \".git\").is_dir()\n    assert (tmp_path / \".gitignore\").exists()\n</code></pre>"},{"location":"how-to/cli-tools/#verifying-file-output","title":"Verifying File Output","text":"<p>Check that CLI operations produce the expected files:</p> <pre><code>async def test_project_scaffold(copilot_run, tmp_path):\n    agent = CopilotAgent(\n        instructions=\"Create project structures as requested.\",\n        working_directory=str(tmp_path),\n    )\n    result = await copilot_run(\n        agent,\n        \"Create a Python package called 'mylib' with __init__.py, \"\n        \"a pyproject.toml using hatchling, and a tests/ directory.\",\n    )\n    assert result.success\n    assert (tmp_path / \"src\" / \"mylib\" / \"__init__.py\").exists() or (\n        tmp_path / \"mylib\" / \"__init__.py\"\n    ).exists()\n    assert (tmp_path / \"pyproject.toml\").exists()\n</code></pre>"},{"location":"how-to/cli-tools/#testing-complex-workflows","title":"Testing Complex Workflows","text":"<p>Chain multiple CLI operations into a single task:</p> <pre><code>async def test_git_workflow(copilot_run, tmp_path):\n    agent = CopilotAgent(\n        instructions=\"Perform git operations as requested. Use git commands directly.\",\n        working_directory=str(tmp_path),\n    )\n    result = await copilot_run(\n        agent,\n        \"Initialize a git repo, create hello.py with print('hello'), \"\n        \"add it, commit with message 'initial', then create a 'feature' branch.\",\n    )\n    assert result.success\n    assert (tmp_path / \"hello.py\").exists()\n</code></pre>"},{"location":"how-to/cli-tools/#comparing-instructions-for-cli-tasks","title":"Comparing Instructions for CLI Tasks","text":"<p>Test which instructions produce better CLI usage:</p> <pre><code>import pytest\nfrom pytest_codingagents import CopilotAgent\n\n\n@pytest.mark.parametrize(\n    \"style,instructions\",\n    [\n        (\"minimal\", \"Execute commands as requested.\"),\n        (\"guided\", \"You are a DevOps assistant. Use standard CLI tools. \"\n         \"Always verify operations succeed before proceeding.\"),\n    ],\n)\nasync def test_cli_instructions(copilot_run, tmp_path, style, instructions):\n    agent = CopilotAgent(\n        name=f\"cli-{style}\",\n        instructions=instructions,\n        working_directory=str(tmp_path),\n    )\n    result = await copilot_run(\n        agent,\n        \"Create a Python virtual environment and install requests\",\n    )\n    assert result.success\n</code></pre>"},{"location":"how-to/copilot-config/","title":"Load from Copilot Config Files","text":"<p><code>CopilotAgent.from_copilot_config()</code> builds a <code>CopilotAgent</code> from any directory that contains GitHub Copilot config files \u2014 your production project, a dedicated test fixture project, a shared team config repo, or anything else.</p>"},{"location":"how-to/copilot-config/#what-it-loads","title":"What it loads","text":"Source Path (relative to the root you point at) Maps to Instructions <code>.github/copilot-instructions.md</code> <code>instructions</code> Custom agents <code>.github/agents/*.agent.md</code> <code>custom_agents</code>"},{"location":"how-to/copilot-config/#basic-usage","title":"Basic usage","text":"<pre><code>from pytest_codingagents import CopilotAgent\n\n# Current directory (default)\nagent = CopilotAgent.from_copilot_config()\n\n# Explicit path\nagent = CopilotAgent.from_copilot_config(\"path/to/any/dir\")\n</code></pre>"},{"location":"how-to/copilot-config/#ab-testing-with-your-production-config","title":"A/B testing with your production config","text":"<p>The main use case: use your real config as the baseline and compare against a variant \u2014 no duplication needed.</p> <pre><code>import pytest\nfrom pytest_codingagents import CopilotAgent\n\n@pytest.fixture\ndef baseline():\n    \"\"\"The actual production Copilot config.\"\"\"\n    return CopilotAgent.from_copilot_config()\n\n@pytest.fixture\ndef treatment():\n    \"\"\"Same config, one instruction changed.\"\"\"\n    return CopilotAgent.from_copilot_config(\n        instructions=\"Always add type hints to every function.\",\n    )\n</code></pre>"},{"location":"how-to/copilot-config/#point-at-any-directory","title":"Point at any directory","text":"<p>There is no concept of \"global\" vs \"project\" \u2014 just a path. Point it wherever your config lives:</p> <pre><code># Production project\nbaseline = CopilotAgent.from_copilot_config(\"/src/my-app\")\n\n# Dedicated test fixture project with stricter agents\ntreatment = CopilotAgent.from_copilot_config(\"tests/fixtures/strict-agents\")\n\n# Shared team config library (checked into a separate repo)\nshared = CopilotAgent.from_copilot_config(\"/shared/team/copilot-config\")\n</code></pre>"},{"location":"how-to/copilot-config/#override-fields-after-loading","title":"Override fields after loading","text":"<p>Any keyword argument overrides the loaded value:</p> <pre><code># Load from the current project but force a specific model\nagent = CopilotAgent.from_copilot_config(model=\"claude-opus-4.5\")\n\n# Load from a different path and override instructions\nagent = CopilotAgent.from_copilot_config(\n    \"tests/fixtures/baseline\",\n    instructions=\"Tightened: always add type hints.\",\n)\n</code></pre>"},{"location":"how-to/copilot-config/#custom-agent-file-format","title":"Custom agent file format","text":"<p>Custom agents are defined in <code>.agent.md</code> files with optional YAML frontmatter:</p> <pre><code>---\nname: test-specialist\ndescription: Focuses on test coverage and quality\ntools:\n  - read\n  - search\n  - edit\n---\n\nYou are a testing specialist. Your responsibilities:\n\n- Analyse existing tests and identify coverage gaps\n- Write unit and integration tests following best practices\n- Focus only on test files; do not modify production code\n</code></pre> <p>The frontmatter supports <code>name</code>, <code>description</code>, <code>tools</code>, and <code>mcp-servers</code>. The Markdown body becomes the agent's prompt.</p>"},{"location":"how-to/copilot-config/#see-also","title":"See also","text":"<ul> <li>A/B Testing Guide</li> <li>GitHub Copilot custom agents docs</li> <li>Custom agents configuration reference</li> </ul>"},{"location":"how-to/mcp-servers/","title":"MCP Server Testing","text":"<p>Test that GitHub Copilot can discover and use your MCP server tools correctly.</p>"},{"location":"how-to/mcp-servers/#basic-usage","title":"Basic Usage","text":"<p>Attach an MCP server to a <code>CopilotAgent</code> and verify the agent calls the right tools:</p> <pre><code>from pytest_codingagents import CopilotAgent\n\n\nasync def test_database_query(copilot_run, tmp_path):\n    agent = CopilotAgent(\n        instructions=\"Use the database tools to answer questions.\",\n        working_directory=str(tmp_path),\n        mcp_servers={\n            \"my-db-server\": {\n                \"command\": \"python\",\n                \"args\": [\"-m\", \"my_db_mcp_server\"],\n            }\n        },\n    )\n    result = await copilot_run(agent, \"List all users in the database\")\n    assert result.success\n    assert result.tool_was_called(\"list_users\")\n</code></pre>"},{"location":"how-to/mcp-servers/#multiple-servers","title":"Multiple Servers","text":"<p>Attach multiple MCP servers to test interactions between tools:</p> <pre><code>async def test_multi_server(copilot_run, tmp_path):\n    agent = CopilotAgent(\n        instructions=\"Use the available tools to complete tasks.\",\n        working_directory=str(tmp_path),\n        mcp_servers={\n            \"database\": {\n                \"command\": \"python\",\n                \"args\": [\"-m\", \"db_server\"],\n            },\n            \"notifications\": {\n                \"command\": \"node\",\n                \"args\": [\"notification_server.js\"],\n            },\n        },\n    )\n    result = await copilot_run(\n        agent,\n        \"Find users who signed up today and send them a welcome notification\",\n    )\n    assert result.success\n    assert result.tool_was_called(\"query_users\")\n    assert result.tool_was_called(\"send_notification\")\n</code></pre>"},{"location":"how-to/mcp-servers/#ab-server-comparison","title":"A/B Server Comparison","text":"<p>Compare two versions of the same MCP server to validate improvements:</p> <pre><code>import pytest\nfrom pytest_codingagents import CopilotAgent\n\nSERVER_VERSIONS = {\n    \"v1\": {\"command\": \"python\", \"args\": [\"-m\", \"my_server_v1\"]},\n    \"v2\": {\"command\": \"python\", \"args\": [\"-m\", \"my_server_v2\"]},\n}\n\n\n@pytest.mark.parametrize(\"version\", SERVER_VERSIONS.keys())\nasync def test_server_version(copilot_run, tmp_path, version):\n    agent = CopilotAgent(\n        name=f\"server-{version}\",\n        instructions=\"Use the available tools to answer questions.\",\n        working_directory=str(tmp_path),\n        mcp_servers={\"my-server\": SERVER_VERSIONS[version]},\n    )\n    result = await copilot_run(agent, \"What's the current inventory count?\")\n    assert result.success\n    assert result.tool_was_called(\"get_inventory\")\n</code></pre> <p>The AI analysis report will compare pass rates and tool usage across server versions, highlighting which performs better.</p>"},{"location":"how-to/mcp-servers/#verifying-tool-arguments","title":"Verifying Tool Arguments","text":"<p>Check not just that a tool was called, but how it was called:</p> <pre><code>async def test_correct_arguments(copilot_run, tmp_path):\n    agent = CopilotAgent(\n        instructions=\"Use database tools to query data.\",\n        working_directory=str(tmp_path),\n        mcp_servers={\n            \"db\": {\"command\": \"python\", \"args\": [\"-m\", \"db_server\"]},\n        },\n    )\n    result = await copilot_run(agent, \"Find users named Alice\")\n    assert result.success\n\n    # Check specific tool calls\n    calls = result.tool_calls_for(\"query_users\")\n    assert len(calls) &gt;= 1\n    assert \"Alice\" in str(calls[0].arguments)\n</code></pre>"},{"location":"how-to/mcp-servers/#environment-variables","title":"Environment Variables","text":"<p>Pass environment variables to your MCP server process:</p> <pre><code>agent = CopilotAgent(\n    instructions=\"Use the API tools.\",\n    working_directory=str(tmp_path),\n    mcp_servers={\n        \"api-server\": {\n            \"command\": \"python\",\n            \"args\": [\"-m\", \"api_server\"],\n            \"env\": {\n                \"API_KEY\": \"test-key\",\n                \"DATABASE_URL\": \"sqlite:///test.db\",\n            },\n        }\n    },\n)\n</code></pre>"},{"location":"how-to/optimize/","title":"Optimizing Instructions with AI","text":"<p><code>optimize_instruction()</code> closes the test\u2192optimize\u2192test loop.</p> <p>When a test fails \u2014 the agent ignored an instruction or produced unexpected output \u2014 call <code>optimize_instruction()</code> to get a concrete, LLM-generated suggestion for improving the instruction. Drop the suggestion into <code>pytest.fail()</code> so the test failure message includes a ready-to-use fix.</p>"},{"location":"how-to/optimize/#the-loop","title":"The Loop","text":"<pre><code>write test \u2192 run \u2192 fail \u2192 optimize \u2192 update instruction \u2192 run \u2192 pass\n</code></pre> <p>This is test-driven prompt engineering: your tests define the standard; the optimizer helps you reach it.</p>"},{"location":"how-to/optimize/#basic-usage","title":"Basic Usage","text":"<pre><code>import pytest\nfrom pytest_codingagents import CopilotAgent, optimize_instruction\n\n\nasync def test_docstring_instruction(copilot_run, tmp_path):\n    agent = CopilotAgent(\n        instructions=\"Write Python code.\",\n        working_directory=str(tmp_path),\n    )\n\n    result = await copilot_run(agent, \"Create math.py with add(a, b) and subtract(a, b).\")\n\n    if '\"\"\"' not in result.file(\"math.py\"):\n        suggestion = await optimize_instruction(\n            agent.instructions or \"\",\n            result,\n            \"Agent should add Google-style docstrings to every function.\",\n        )\n        pytest.fail(f\"No docstrings found.\\n\\n{suggestion}\")\n</code></pre> <p>The failure message will look like:</p> <pre><code>FAILED test_math.py::test_docstring_instruction\n\nNo docstrings found.\n\n\ud83d\udca1 Suggested instruction:\n\n  Write Python code. Add Google-style docstrings to every function.\n  The docstring should describe what the function does, its parameters (Args:),\n  and its return value (Returns:).\n\n  Changes: Added explicit docstring format mandate with Args/Returns sections.\n  Reasoning: The original instruction did not mention documentation. The agent\n  produced code without docstrings because there was no requirement to add them.\n</code></pre>"},{"location":"how-to/optimize/#with-ab-testing","title":"With A/B Testing","text":"<p>Pair <code>optimize_instruction()</code> with <code>ab_run</code> to test the fix before committing:</p> <pre><code>import pytest\nfrom pytest_codingagents import CopilotAgent, optimize_instruction\n\n\nasync def test_docstring_instruction_iterates(ab_run, tmp_path):\n    baseline = CopilotAgent(instructions=\"Write Python code.\")\n    treatment = CopilotAgent(\n        instructions=\"Write Python code. Add Google-style docstrings to every function.\"\n    )\n\n    b, t = await ab_run(baseline, treatment, \"Create math.py with add(a, b).\")\n\n    assert b.success and t.success\n\n    if '\"\"\"' not in t.file(\"math.py\"):\n        suggestion = await optimize_instruction(\n            treatment.instructions or \"\",\n            t,\n            \"Treatment agent should add docstrings \u2014 treatment instruction did not work.\",\n        )\n        pytest.fail(f\"Treatment still no docstrings.\\n\\n{suggestion}\")\n\n    # Confirm baseline does NOT have docstrings (differential assertion)\n    assert '\"\"\"' not in b.file(\"math.py\"), \"Baseline unexpectedly has docstrings\"\n</code></pre>"},{"location":"how-to/optimize/#api-reference","title":"API Reference","text":""},{"location":"how-to/optimize/#pytest_codingagents.copilot.optimizer.optimize_instruction","title":"<code>optimize_instruction(current_instruction: str, result: CopilotResult, criterion: str, *, model: str = 'openai:gpt-4o-mini') -&gt; InstructionSuggestion</code>  <code>async</code>","text":"<p>Analyze a result and suggest an improved instruction.</p> <p>Uses pydantic-ai structured output to analyze the gap between a current instruction and the agent's observed behavior, returning a concrete, actionable improvement.</p> <p>Designed to drop into <code>pytest.fail()</code> so the failure message contains a ready-to-use fix:</p> <p>Example::</p> <pre><code>result = await copilot_run(agent, task)\nif '\"\"\"' not in result.file(\"main.py\"):\n    suggestion = await optimize_instruction(\n        agent.instructions or \"\",\n        result,\n        \"Agent should add docstrings to all functions.\",\n    )\n    pytest.fail(f\"No docstrings found.\\n\\n{suggestion}\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>current_instruction</code> <code>str</code> <p>The agent's current instruction text.</p> required <code>result</code> <code>CopilotResult</code> <p>The <code>CopilotResult</code> from the (failed) run.</p> required <code>criterion</code> <code>str</code> <p>What the agent should have done \u2014 the test expectation in plain English (e.g. <code>\"Always write docstrings\"</code>).</p> required <code>model</code> <code>str</code> <p>LiteLLM-style model string (e.g. <code>\"openai:gpt-4o-mini\"</code> or <code>\"anthropic:claude-3-haiku-20240307\"</code>).</p> <code>'openai:gpt-4o-mini'</code> <p>Returns:</p> Name Type Description <code>An</code> <code>InstructionSuggestion</code> <p>class:<code>InstructionSuggestion</code> with the improved instruction.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If pydantic-ai is not installed.</p>"},{"location":"how-to/optimize/#pytest_codingagents.copilot.optimizer.InstructionSuggestion","title":"<code>InstructionSuggestion(instruction: str, reasoning: str, changes: str)</code>  <code>dataclass</code>","text":"<p>A suggested improvement to a Copilot agent instruction.</p> <p>Returned by :func:<code>optimize_instruction</code>. Designed to drop into <code>pytest.fail()</code> so the failure message includes an actionable fix.</p> <p>Attributes:</p> Name Type Description <code>instruction</code> <code>str</code> <p>The improved instruction text to use instead.</p> <code>reasoning</code> <code>str</code> <p>Explanation of why this change would close the gap.</p> <code>changes</code> <code>str</code> <p>Short description of what was changed (one sentence).</p> <p>Example::</p> <pre><code>suggestion = await optimize_instruction(\n    agent.instructions,\n    result,\n    \"Agent should add docstrings to all functions.\",\n)\npytest.fail(f\"No docstrings found.\\n\\n{suggestion}\")\n</code></pre>"},{"location":"how-to/optimize/#choosing-a-model","title":"Choosing a Model","text":"<p><code>optimize_instruction()</code> defaults to <code>openai:gpt-4o-mini</code> \u2014 cheap, fast, and precise enough for instruction analysis.</p> <p>Override with the <code>model</code> keyword argument:</p> <pre><code>suggestion = await optimize_instruction(\n    agent.instructions or \"\",\n    result,\n    \"Agent should use type hints.\",\n    model=\"anthropic:claude-3-haiku-20240307\",\n)\n</code></pre> <p>Any LiteLLM-compatible model string works.</p>"},{"location":"how-to/optimize/#the-criterion","title":"The Criterion","text":"<p>Write the <code>criterion</code> as a plain-English statement of what the agent should have done:</p> Situation Good criterion Missing docstrings <code>\"Agent should add Google-style docstrings to every function.\"</code> Wrong framework <code>\"Agent should use FastAPI, not Flask.\"</code> Missing type hints <code>\"All function signatures must include type annotations.\"</code> No error handling <code>\"All I/O operations must be wrapped in try/except.\"</code> <p>The more specific the criterion, the more actionable the suggestion.</p>"},{"location":"how-to/skills/","title":"Skill Testing","text":"<p>Test whether domain knowledge injected via skill directories improves agent behavior.</p>"},{"location":"how-to/skills/#what-are-skills","title":"What Are Skills?","text":"<p>Skills are directories containing markdown files with domain-specific knowledge \u2014 coding standards, API conventions, architectural guidelines, etc. When attached to a <code>CopilotAgent</code>, these files are loaded into the agent's context.</p>"},{"location":"how-to/skills/#basic-usage","title":"Basic Usage","text":"<p>Create a skill directory and test that the agent follows its guidance:</p> <pre><code>from pytest_codingagents import CopilotAgent\n\n\nasync def test_coding_standards(copilot_run, tmp_path):\n    # Create a skill with coding standards\n    skill_dir = tmp_path / \"skills\"\n    skill_dir.mkdir()\n    (skill_dir / \"coding-standards.md\").write_text(\n        \"# Coding Standards\\n\\n\"\n        \"All Python functions MUST have type hints and docstrings.\\n\"\n        \"Use snake_case for all function names.\\n\"\n    )\n\n    agent = CopilotAgent(\n        name=\"with-standards\",\n        instructions=\"Follow all coding standards from your skills.\",\n        working_directory=str(tmp_path),\n        skill_directories=[str(skill_dir)],\n    )\n    result = await copilot_run(agent, \"Create math_utils.py with add and multiply\")\n    assert result.success\n    content = (tmp_path / \"math_utils.py\").read_text()\n    assert \"def add\" in content\n</code></pre>"},{"location":"how-to/skills/#measuring-skill-impact","title":"Measuring Skill Impact","text":"<p>Parametrize tests with and without skills to measure whether they improve results:</p> <pre><code>import pytest\nfrom pytest_codingagents import CopilotAgent\n\n\n@pytest.mark.parametrize(\"use_skill\", [True, False], ids=[\"with-skill\", \"without-skill\"])\nasync def test_skill_impact(copilot_run, tmp_path, use_skill):\n    # Set up skill directory\n    skill_dir = tmp_path / \"skills\"\n    skill_dir.mkdir()\n    (skill_dir / \"standards.md\").write_text(\n        \"All functions MUST have type hints and docstrings.\"\n    )\n\n    skill_dirs = [str(skill_dir)] if use_skill else []\n    agent = CopilotAgent(\n        name=f\"skill-{'on' if use_skill else 'off'}\",\n        instructions=\"Create clean Python code.\",\n        working_directory=str(tmp_path),\n        skill_directories=skill_dirs,\n    )\n    result = await copilot_run(agent, \"Create a utility module with 3 helper functions\")\n    assert result.success\n</code></pre> <p>The AI analysis report highlights differences in behavior and quality between skill-on and skill-off runs.</p>"},{"location":"how-to/skills/#multiple-skill-directories","title":"Multiple Skill Directories","text":"<p>Load skills from several directories:</p> <pre><code>agent = CopilotAgent(\n    instructions=\"Follow all project guidelines.\",\n    working_directory=str(tmp_path),\n    skill_directories=[\n        \"skills/coding-standards\",\n        \"skills/api-conventions\",\n        \"skills/testing-patterns\",\n    ],\n)\n</code></pre>"},{"location":"how-to/skills/#disabling-specific-skills","title":"Disabling Specific Skills","text":"<p>Selectively disable skills to isolate their effects:</p> <pre><code>agent = CopilotAgent(\n    instructions=\"Follow project guidelines.\",\n    working_directory=str(tmp_path),\n    skill_directories=[\"skills/\"],\n    disabled_skills=[\"deprecated-patterns\"],\n)\n</code></pre>"},{"location":"how-to/skills/#comparing-skill-versions","title":"Comparing Skill Versions","text":"<p>Test different versions of the same skill to find the most effective phrasing:</p> <pre><code>import pytest\nfrom pytest_codingagents import CopilotAgent\n\nSKILL_VERSIONS = {\n    \"concise\": \"Use type hints. Use docstrings. Use snake_case.\",\n    \"detailed\": (\n        \"# Coding Standards\\n\\n\"\n        \"## Type Hints\\n\"\n        \"Every function parameter and return value MUST have a type hint.\\n\\n\"\n        \"## Docstrings\\n\"\n        \"Every public function MUST have a Google-style docstring.\\n\"\n    ),\n}\n\n\n@pytest.mark.parametrize(\"style\", SKILL_VERSIONS.keys())\nasync def test_skill_phrasing(copilot_run, tmp_path, style):\n    skill_dir = tmp_path / \"skills\"\n    skill_dir.mkdir()\n    (skill_dir / \"standards.md\").write_text(SKILL_VERSIONS[style])\n\n    agent = CopilotAgent(\n        name=f\"skill-{style}\",\n        instructions=\"Follow the coding standards.\",\n        working_directory=str(tmp_path),\n        skill_directories=[str(skill_dir)],\n    )\n    result = await copilot_run(agent, \"Create a data processing module\")\n    assert result.success\n</code></pre>"},{"location":"how-to/tool-control/","title":"Tool Control","text":"<p>Restrict which tools the agent can use with allowlists and blocklists.</p>"},{"location":"how-to/tool-control/#allowlist-whitelist","title":"Allowlist (Whitelist)","text":"<p>Only permit specific tools \u2014 the agent cannot use anything else:</p> <pre><code>from pytest_codingagents import CopilotAgent\n\n\nasync def test_read_only(copilot_run, tmp_path):\n    agent = CopilotAgent(\n        instructions=\"Answer questions about the codebase.\",\n        working_directory=str(tmp_path),\n        allowed_tools=[\"read_file\", \"grep_search\", \"list_dir\"],\n    )\n    result = await copilot_run(agent, \"What files are in this directory?\")\n    assert result.success\n    # Agent can only read \u2014 no file creation or modification\n</code></pre>"},{"location":"how-to/tool-control/#blocklist-blacklist","title":"Blocklist (Blacklist)","text":"<p>Block specific tools while allowing everything else:</p> <pre><code>async def test_no_write(copilot_run, tmp_path):\n    agent = CopilotAgent(\n        instructions=\"Review code without modifying it.\",\n        working_directory=str(tmp_path),\n        excluded_tools=[\"create_file\", \"replace_string_in_file\", \"run_in_terminal\"],\n    )\n    result = await copilot_run(agent, \"Review this project for potential bugs\")\n    assert result.success\n</code></pre>"},{"location":"how-to/tool-control/#comparing-tool-restrictions","title":"Comparing Tool Restrictions","text":"<p>Test whether restricting tools changes agent behavior:</p> <pre><code>import pytest\nfrom pytest_codingagents import CopilotAgent\n\n\n@pytest.mark.parametrize(\n    \"mode,allowed,excluded\",\n    [\n        (\"unrestricted\", None, None),\n        (\"read-only\", [\"read_file\", \"grep_search\", \"list_dir\"], None),\n        (\"no-terminal\", None, [\"run_in_terminal\"]),\n    ],\n)\nasync def test_tool_restrictions(copilot_run, tmp_path, mode, allowed, excluded):\n    agent = CopilotAgent(\n        name=f\"tools-{mode}\",\n        instructions=\"Complete the task using available tools.\",\n        working_directory=str(tmp_path),\n        allowed_tools=allowed,\n        excluded_tools=excluded,\n    )\n    result = await copilot_run(agent, \"Create a hello.py file\")\n    # In read-only mode, this should fail (no create_file tool)\n    if mode == \"read-only\":\n        assert not result.tool_was_called(\"create_file\")\n    else:\n        assert result.success\n</code></pre>"},{"location":"how-to/tool-control/#use-cases","title":"Use Cases","text":"Scenario Configuration Code review agent <code>allowed_tools=[\"read_file\", \"grep_search\", \"list_dir\"]</code> File creation only <code>allowed_tools=[\"create_file\", \"read_file\"]</code> No terminal access <code>excluded_tools=[\"run_in_terminal\"]</code> No file modification <code>excluded_tools=[\"create_file\", \"replace_string_in_file\"]</code>"},{"location":"how-to/tool-control/#notes","title":"Notes","text":"<ul> <li><code>allowed_tools</code> and <code>excluded_tools</code> are mutually exclusive \u2014 use one or the other</li> <li>When <code>allowed_tools</code> is <code>None</code> (default), all tools are available</li> <li>Tool names correspond to the Copilot SDK tool names (e.g., <code>create_file</code>, <code>read_file</code>, <code>run_in_terminal</code>)</li> </ul>"},{"location":"reference/agent/","title":"CopilotAgent","text":""},{"location":"reference/agent/#pytest_codingagents.copilot.agent.CopilotAgent","title":"<code>CopilotAgent(name: str = 'copilot', model: str | None = None, reasoning_effort: Literal['low', 'medium', 'high', 'xhigh'] | None = None, instructions: str | None = None, system_message_mode: Literal['append', 'replace'] = 'append', working_directory: str | None = None, allowed_tools: list[str] | None = None, excluded_tools: list[str] | None = None, max_turns: int = 25, timeout_s: float = 300.0, max_retries: int = 2, retry_delay_s: float = 5.0, auto_confirm: bool = True, mcp_servers: dict[str, Any] = dict(), custom_agents: list[dict[str, Any]] = list(), skill_directories: list[str] = list(), disabled_skills: list[str] = list(), extra_config: dict[str, Any] = dict())</code>  <code>dataclass</code>","text":"<p>Configuration for a GitHub Copilot agent test.</p> <p>Maps to the Copilot SDK's <code>SessionConfig</code>. User-facing field names are kept intuitive (e.g. <code>instructions</code>), while <code>build_session_config()</code> maps them to the SDK's actual <code>system_message</code> TypedDict.</p> <p>The SDK's <code>SessionConfig</code> has no <code>maxTurns</code> field \u2014 turn limits are enforced externally by the runner via <code>timeout_s</code>.</p> Example"},{"location":"reference/agent/#pytest_codingagents.copilot.agent.CopilotAgent--minimal","title":"Minimal","text":"<p>CopilotAgent()</p>"},{"location":"reference/agent/#pytest_codingagents.copilot.agent.CopilotAgent--with-instructions-and-model","title":"With instructions and model","text":"<p>CopilotAgent(     name=\"security-reviewer\",     model=\"claude-sonnet-4\",     instructions=\"Review code for security vulnerabilities.\", )</p>"},{"location":"reference/agent/#pytest_codingagents.copilot.agent.CopilotAgent--with-custom-tools-and-references","title":"With custom tools and references","text":"<p>CopilotAgent(     name=\"file-creator\",     instructions=\"Create files as requested.\",     working_directory=\"/tmp/workspace\",     allowed_tools=[\"create_file\", \"read_file\"], )</p>"},{"location":"reference/agent/#pytest_codingagents.copilot.agent.CopilotAgent.from_copilot_config","title":"<code>from_copilot_config(path: str | Path = '.', **overrides: Any) -&gt; 'CopilotAgent'</code>  <code>classmethod</code>","text":"<p>Load a <code>CopilotAgent</code> from a directory containing Copilot config files.</p> <p>Looks for the following files under <code>path</code>:</p> <ul> <li><code>.github/copilot-instructions.md</code> \u2192 <code>instructions</code></li> <li><code>.github/agents/*.agent.md</code> \u2192 <code>custom_agents</code></li> </ul> <p>Point <code>path</code> at any directory \u2014 your production project, a dedicated test fixture project, or a shared team config repo.  Any keyword argument overrides the loaded value.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Root directory to load config from. Defaults to the current working directory.</p> <code>'.'</code> <code>**overrides</code> <code>Any</code> <p>Override any <code>CopilotAgent</code> field after loading, e.g. <code>model=\"claude-opus-4.5\"</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>'CopilotAgent'</code> <p>A <code>CopilotAgent</code> initialised from the discovered config files.</p> <p>Example::</p> <pre><code># Load from the current project (production config as baseline)\nbaseline = CopilotAgent.from_copilot_config()\n\n# A/B test: same config, one instruction changed\ntreatment = CopilotAgent.from_copilot_config(\n    instructions=\"Always add type hints.\",\n)\n\n# Load from a dedicated test-fixture project\nagent = CopilotAgent.from_copilot_config(\"tests/fixtures/strict-agent\")\n\n# Load from a shared team agent library\nagent = CopilotAgent.from_copilot_config(\"/shared/team/copilot-config\")\n</code></pre>"},{"location":"reference/agent/#pytest_codingagents.copilot.agent.CopilotAgent.build_session_config","title":"<code>build_session_config() -&gt; dict[str, Any]</code>","text":"<p>Build a SessionConfig dict for the Copilot SDK.</p> <p>Returns a dict compatible with <code>CopilotClient.create_session()</code>. Only includes non-None/non-default fields to avoid overriding SDK defaults.</p> <p>SDK field mapping (Python snake_case TypedDict keys):     instructions \u2192 system_message: {mode, content}     allowed_tools \u2192 available_tools     excluded_tools \u2192 excluded_tools     reasoning_effort \u2192 reasoning_effort     working_directory \u2192 working_directory     mcp_servers \u2192 mcp_servers     custom_agents \u2192 custom_agents     skill_directories \u2192 skill_directories     disabled_skills \u2192 disabled_skills</p> <p>Note: <code>max_turns</code> is NOT part of <code>SessionConfig</code> \u2014 the runner enforces turn limits externally.</p>"},{"location":"reference/api/","title":"API Reference","text":""},{"location":"reference/api/#pytest_codingagents.CopilotAgent","title":"<code>CopilotAgent(name: str = 'copilot', model: str | None = None, reasoning_effort: Literal['low', 'medium', 'high', 'xhigh'] | None = None, instructions: str | None = None, system_message_mode: Literal['append', 'replace'] = 'append', working_directory: str | None = None, allowed_tools: list[str] | None = None, excluded_tools: list[str] | None = None, max_turns: int = 25, timeout_s: float = 300.0, max_retries: int = 2, retry_delay_s: float = 5.0, auto_confirm: bool = True, mcp_servers: dict[str, Any] = dict(), custom_agents: list[dict[str, Any]] = list(), skill_directories: list[str] = list(), disabled_skills: list[str] = list(), extra_config: dict[str, Any] = dict())</code>  <code>dataclass</code>","text":"<p>Configuration for a GitHub Copilot agent test.</p> <p>Maps to the Copilot SDK's <code>SessionConfig</code>. User-facing field names are kept intuitive (e.g. <code>instructions</code>), while <code>build_session_config()</code> maps them to the SDK's actual <code>system_message</code> TypedDict.</p> <p>The SDK's <code>SessionConfig</code> has no <code>maxTurns</code> field \u2014 turn limits are enforced externally by the runner via <code>timeout_s</code>.</p> Example"},{"location":"reference/api/#pytest_codingagents.CopilotAgent--minimal","title":"Minimal","text":"<p>CopilotAgent()</p>"},{"location":"reference/api/#pytest_codingagents.CopilotAgent--with-instructions-and-model","title":"With instructions and model","text":"<p>CopilotAgent(     name=\"security-reviewer\",     model=\"claude-sonnet-4\",     instructions=\"Review code for security vulnerabilities.\", )</p>"},{"location":"reference/api/#pytest_codingagents.CopilotAgent--with-custom-tools-and-references","title":"With custom tools and references","text":"<p>CopilotAgent(     name=\"file-creator\",     instructions=\"Create files as requested.\",     working_directory=\"/tmp/workspace\",     allowed_tools=[\"create_file\", \"read_file\"], )</p>"},{"location":"reference/api/#pytest_codingagents.CopilotAgent.build_session_config","title":"<code>build_session_config() -&gt; dict[str, Any]</code>","text":"<p>Build a SessionConfig dict for the Copilot SDK.</p> <p>Returns a dict compatible with <code>CopilotClient.create_session()</code>. Only includes non-None/non-default fields to avoid overriding SDK defaults.</p> <p>SDK field mapping (Python snake_case TypedDict keys):     instructions \u2192 system_message: {mode, content}     allowed_tools \u2192 available_tools     excluded_tools \u2192 excluded_tools     reasoning_effort \u2192 reasoning_effort     working_directory \u2192 working_directory     mcp_servers \u2192 mcp_servers     custom_agents \u2192 custom_agents     skill_directories \u2192 skill_directories     disabled_skills \u2192 disabled_skills</p> <p>Note: <code>max_turns</code> is NOT part of <code>SessionConfig</code> \u2014 the runner enforces turn limits externally.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotAgent.from_copilot_config","title":"<code>from_copilot_config(path: str | Path = '.', **overrides: Any) -&gt; 'CopilotAgent'</code>  <code>classmethod</code>","text":"<p>Load a <code>CopilotAgent</code> from a directory containing Copilot config files.</p> <p>Looks for the following files under <code>path</code>:</p> <ul> <li><code>.github/copilot-instructions.md</code> \u2192 <code>instructions</code></li> <li><code>.github/agents/*.agent.md</code> \u2192 <code>custom_agents</code></li> </ul> <p>Point <code>path</code> at any directory \u2014 your production project, a dedicated test fixture project, or a shared team config repo.  Any keyword argument overrides the loaded value.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Root directory to load config from. Defaults to the current working directory.</p> <code>'.'</code> <code>**overrides</code> <code>Any</code> <p>Override any <code>CopilotAgent</code> field after loading, e.g. <code>model=\"claude-opus-4.5\"</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>'CopilotAgent'</code> <p>A <code>CopilotAgent</code> initialised from the discovered config files.</p> <p>Example::</p> <pre><code># Load from the current project (production config as baseline)\nbaseline = CopilotAgent.from_copilot_config()\n\n# A/B test: same config, one instruction changed\ntreatment = CopilotAgent.from_copilot_config(\n    instructions=\"Always add type hints.\",\n)\n\n# Load from a dedicated test-fixture project\nagent = CopilotAgent.from_copilot_config(\"tests/fixtures/strict-agent\")\n\n# Load from a shared team agent library\nagent = CopilotAgent.from_copilot_config(\"/shared/team/copilot-config\")\n</code></pre>"},{"location":"reference/api/#pytest_codingagents.CopilotResult","title":"<code>CopilotResult(turns: list[Turn] = list(), success: bool = True, error: str | None = None, duration_ms: float = 0.0, usage: list[UsageInfo] = list(), reasoning_traces: list[str] = list(), subagent_invocations: list[SubagentInvocation] = list(), permission_requested: bool = False, permissions: list[dict[str, Any]] = list(), model_used: str | None = None, raw_events: list[Any] = list(), agent: CopilotAgent | None = None)</code>  <code>dataclass</code>","text":"<p>Result of running a prompt against GitHub Copilot.</p> <p>Captures the full event stream from the SDK, including tool calls, reasoning traces, subagent routing, permissions, and token usage.</p> Example <p>result = await copilot_run(agent, \"Create hello.py\") assert result.success assert result.tool_was_called(\"create_file\") assert \"hello\" in result.final_response.lower()</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.final_response","title":"<code>final_response: str | None</code>  <code>property</code>","text":"<p>Get the last assistant response.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.all_responses","title":"<code>all_responses: list[str]</code>  <code>property</code>","text":"<p>Get all assistant responses.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.all_tool_calls","title":"<code>all_tool_calls: list[ToolCall]</code>  <code>property</code>","text":"<p>Get all tool calls across all turns.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.tool_names_called","title":"<code>tool_names_called: set[str]</code>  <code>property</code>","text":"<p>Get set of all tool names that were called.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.total_input_tokens","title":"<code>total_input_tokens: int</code>  <code>property</code>","text":"<p>Total input tokens across all model turns.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.total_output_tokens","title":"<code>total_output_tokens: int</code>  <code>property</code>","text":"<p>Total output tokens across all model turns.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.total_tokens","title":"<code>total_tokens: int</code>  <code>property</code>","text":"<p>Total tokens (input + output) across all model turns.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.total_cost_usd","title":"<code>total_cost_usd: float</code>  <code>property</code>","text":"<p>Total cost in USD across all model turns.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.token_usage","title":"<code>token_usage: dict[str, int]</code>  <code>property</code>","text":"<p>Token usage dict compatible with pytest-aitest's AgentResult.</p> <p>Keys use short names (<code>prompt</code>, <code>completion</code>, <code>total</code>) to match the format pytest-aitest reads in its collector and generator.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.cost_usd","title":"<code>cost_usd: float</code>  <code>property</code>","text":"<p>Cost in USD, compatible with pytest-aitest's AgentResult.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.working_directory","title":"<code>working_directory: Path</code>  <code>property</code>","text":"<p>Working directory where the agent operated.</p> <p>Resolved from <code>agent.working_directory</code> when set; falls back to the current working directory.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.tool_was_called","title":"<code>tool_was_called(name: str) -&gt; bool</code>","text":"<p>Check if a specific tool was called.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.tool_call_count","title":"<code>tool_call_count(name: str) -&gt; int</code>","text":"<p>Count how many times a specific tool was called.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.tool_calls_for","title":"<code>tool_calls_for(name: str) -&gt; list[ToolCall]</code>","text":"<p>Get all calls to a specific tool.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.file","title":"<code>file(path: str) -&gt; str</code>","text":"<p>Read the content of a file relative to the working directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative file path (e.g. <code>\"main.py\"</code> or <code>\"src/utils.py\"</code>).</p> required <p>Returns:</p> Type Description <code>str</code> <p>File content as a string.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.file_exists","title":"<code>file_exists(path: str) -&gt; bool</code>","text":"<p>Check whether a file exists in the working directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the file exists, <code>False</code> otherwise.</p>"},{"location":"reference/api/#pytest_codingagents.CopilotResult.files_matching","title":"<code>files_matching(pattern: str = '**/*') -&gt; list[Path]</code>","text":"<p>Find files matching a glob pattern in the working directory.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Glob pattern relative to the working directory. Defaults to <code>\"**/*\"</code> (all files recursively).</p> <code>'**/*'</code> <p>Returns:</p> Type Description <code>list[Path]</code> <p>Sorted list of matching <code>Path</code> objects (files only, no</p> <code>list[Path]</code> <p>directories).</p> <p>Example::</p> <pre><code># All Python files created by the agent\npy_files = result.files_matching(\"**/*.py\")\nassert py_files, \"No Python files were created\"\n\n# Top-level test files\ntest_files = result.files_matching(\"test_*.py\")\n</code></pre>"},{"location":"reference/api/#pytest_codingagents.optimize_instruction","title":"<code>optimize_instruction(current_instruction: str, result: CopilotResult, criterion: str, *, model: str = 'openai:gpt-4o-mini') -&gt; InstructionSuggestion</code>  <code>async</code>","text":"<p>Analyze a result and suggest an improved instruction.</p> <p>Uses pydantic-ai structured output to analyze the gap between a current instruction and the agent's observed behavior, returning a concrete, actionable improvement.</p> <p>Designed to drop into <code>pytest.fail()</code> so the failure message contains a ready-to-use fix:</p> <p>Example::</p> <pre><code>result = await copilot_run(agent, task)\nif '\"\"\"' not in result.file(\"main.py\"):\n    suggestion = await optimize_instruction(\n        agent.instructions or \"\",\n        result,\n        \"Agent should add docstrings to all functions.\",\n    )\n    pytest.fail(f\"No docstrings found.\\n\\n{suggestion}\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>current_instruction</code> <code>str</code> <p>The agent's current instruction text.</p> required <code>result</code> <code>CopilotResult</code> <p>The <code>CopilotResult</code> from the (failed) run.</p> required <code>criterion</code> <code>str</code> <p>What the agent should have done \u2014 the test expectation in plain English (e.g. <code>\"Always write docstrings\"</code>).</p> required <code>model</code> <code>str</code> <p>LiteLLM-style model string (e.g. <code>\"openai:gpt-4o-mini\"</code> or <code>\"anthropic:claude-3-haiku-20240307\"</code>).</p> <code>'openai:gpt-4o-mini'</code> <p>Returns:</p> Name Type Description <code>An</code> <code>InstructionSuggestion</code> <p>class:<code>InstructionSuggestion</code> with the improved instruction.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If pydantic-ai is not installed.</p>"},{"location":"reference/api/#pytest_codingagents.InstructionSuggestion","title":"<code>InstructionSuggestion(instruction: str, reasoning: str, changes: str)</code>  <code>dataclass</code>","text":"<p>A suggested improvement to a Copilot agent instruction.</p> <p>Returned by :func:<code>optimize_instruction</code>. Designed to drop into <code>pytest.fail()</code> so the failure message includes an actionable fix.</p> <p>Attributes:</p> Name Type Description <code>instruction</code> <code>str</code> <p>The improved instruction text to use instead.</p> <code>reasoning</code> <code>str</code> <p>Explanation of why this change would close the gap.</p> <code>changes</code> <code>str</code> <p>Short description of what was changed (one sentence).</p> <p>Example::</p> <pre><code>suggestion = await optimize_instruction(\n    agent.instructions,\n    result,\n    \"Agent should add docstrings to all functions.\",\n)\npytest.fail(f\"No docstrings found.\\n\\n{suggestion}\")\n</code></pre>"},{"location":"reference/configuration/","title":"Configuration","text":""},{"location":"reference/configuration/#copilotagent-fields","title":"CopilotAgent Fields","text":"Field Type Default Description <code>name</code> <code>str</code> <code>\"copilot\"</code> Agent identifier for reports <code>model</code> <code>str \\| None</code> <code>None</code> Model to use (e.g., <code>claude-sonnet-4</code>) <code>instructions</code> <code>str \\| None</code> <code>None</code> Instructions for the agent <code>system_message_mode</code> <code>Literal[\"append\", \"replace\"]</code> <code>\"append\"</code> <code>\"append\"</code> adds to Copilot's built-in system message; <code>\"replace\"</code> overrides it <code>working_directory</code> <code>str \\| None</code> <code>None</code> Working directory for file operations <code>max_turns</code> <code>int</code> <code>25</code> Maximum conversation turns (informational \u2014 enforced via <code>timeout_s</code>, not in SDK) <code>timeout_s</code> <code>float</code> <code>300.0</code> Timeout in seconds <code>auto_confirm</code> <code>bool</code> <code>True</code> Auto-approve tool permissions <code>reasoning_effort</code> <code>Literal[\"low\", \"medium\", \"high\", \"xhigh\"] \\| None</code> <code>None</code> Reasoning effort level <code>allowed_tools</code> <code>list[str] \\| None</code> <code>None</code> Allowlist of tools <code>excluded_tools</code> <code>list[str] \\| None</code> <code>None</code> Blocklist of tools <code>mcp_servers</code> <code>dict</code> <code>{}</code> MCP server configurations <code>custom_agents</code> <code>list[dict]</code> <code>[]</code> Custom agent configurations <code>skill_directories</code> <code>list[str]</code> <code>[]</code> Paths to skill directories <code>disabled_skills</code> <code>list[str]</code> <code>[]</code> Skills to disable <code>extra_config</code> <code>dict</code> <code>{}</code> SDK passthrough for unmapped fields"},{"location":"reference/configuration/#authentication","title":"Authentication","text":"<p>Authentication is resolved in order:</p> <ol> <li><code>GITHUB_TOKEN</code> environment variable (ideal for CI)</li> <li>Logged-in user via <code>gh</code> CLI / OAuth (local development)</li> </ol>"},{"location":"reference/configuration/#pytest-configuration","title":"pytest Configuration","text":"<p>Add to <code>pyproject.toml</code>:</p> <pre><code>[tool.pytest.ini_options]\nasyncio_mode = \"auto\"\nmarkers = [\n    \"copilot: marks tests as requiring GitHub Copilot SDK credentials\",\n]\n</code></pre>"},{"location":"reference/result/","title":"CopilotResult","text":""},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult","title":"<code>CopilotResult(turns: list[Turn] = list(), success: bool = True, error: str | None = None, duration_ms: float = 0.0, usage: list[UsageInfo] = list(), reasoning_traces: list[str] = list(), subagent_invocations: list[SubagentInvocation] = list(), permission_requested: bool = False, permissions: list[dict[str, Any]] = list(), model_used: str | None = None, raw_events: list[Any] = list(), agent: CopilotAgent | None = None)</code>  <code>dataclass</code>","text":"<p>Result of running a prompt against GitHub Copilot.</p> <p>Captures the full event stream from the SDK, including tool calls, reasoning traces, subagent routing, permissions, and token usage.</p> Example <p>result = await copilot_run(agent, \"Create hello.py\") assert result.success assert result.tool_was_called(\"create_file\") assert \"hello\" in result.final_response.lower()</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.final_response","title":"<code>final_response: str | None</code>  <code>property</code>","text":"<p>Get the last assistant response.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.all_responses","title":"<code>all_responses: list[str]</code>  <code>property</code>","text":"<p>Get all assistant responses.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.all_tool_calls","title":"<code>all_tool_calls: list[ToolCall]</code>  <code>property</code>","text":"<p>Get all tool calls across all turns.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.tool_names_called","title":"<code>tool_names_called: set[str]</code>  <code>property</code>","text":"<p>Get set of all tool names that were called.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.total_input_tokens","title":"<code>total_input_tokens: int</code>  <code>property</code>","text":"<p>Total input tokens across all model turns.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.total_output_tokens","title":"<code>total_output_tokens: int</code>  <code>property</code>","text":"<p>Total output tokens across all model turns.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.total_tokens","title":"<code>total_tokens: int</code>  <code>property</code>","text":"<p>Total tokens (input + output) across all model turns.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.total_cost_usd","title":"<code>total_cost_usd: float</code>  <code>property</code>","text":"<p>Total cost in USD across all model turns.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.token_usage","title":"<code>token_usage: dict[str, int]</code>  <code>property</code>","text":"<p>Token usage dict compatible with pytest-aitest's AgentResult.</p> <p>Keys use short names (<code>prompt</code>, <code>completion</code>, <code>total</code>) to match the format pytest-aitest reads in its collector and generator.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.cost_usd","title":"<code>cost_usd: float</code>  <code>property</code>","text":"<p>Cost in USD, compatible with pytest-aitest's AgentResult.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.working_directory","title":"<code>working_directory: Path</code>  <code>property</code>","text":"<p>Working directory where the agent operated.</p> <p>Resolved from <code>agent.working_directory</code> when set; falls back to the current working directory.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.tool_was_called","title":"<code>tool_was_called(name: str) -&gt; bool</code>","text":"<p>Check if a specific tool was called.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.tool_call_count","title":"<code>tool_call_count(name: str) -&gt; int</code>","text":"<p>Count how many times a specific tool was called.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.tool_calls_for","title":"<code>tool_calls_for(name: str) -&gt; list[ToolCall]</code>","text":"<p>Get all calls to a specific tool.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.file","title":"<code>file(path: str) -&gt; str</code>","text":"<p>Read the content of a file relative to the working directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative file path (e.g. <code>\"main.py\"</code> or <code>\"src/utils.py\"</code>).</p> required <p>Returns:</p> Type Description <code>str</code> <p>File content as a string.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.file_exists","title":"<code>file_exists(path: str) -&gt; bool</code>","text":"<p>Check whether a file exists in the working directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative file path.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the file exists, <code>False</code> otherwise.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.CopilotResult.files_matching","title":"<code>files_matching(pattern: str = '**/*') -&gt; list[Path]</code>","text":"<p>Find files matching a glob pattern in the working directory.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Glob pattern relative to the working directory. Defaults to <code>\"**/*\"</code> (all files recursively).</p> <code>'**/*'</code> <p>Returns:</p> Type Description <code>list[Path]</code> <p>Sorted list of matching <code>Path</code> objects (files only, no</p> <code>list[Path]</code> <p>directories).</p> <p>Example::</p> <pre><code># All Python files created by the agent\npy_files = result.files_matching(\"**/*.py\")\nassert py_files, \"No Python files were created\"\n\n# Top-level test files\ntest_files = result.files_matching(\"test_*.py\")\n</code></pre>"},{"location":"reference/result/#pytest_codingagents.copilot.result.UsageInfo","title":"<code>UsageInfo(model: str, input_tokens: int = 0, output_tokens: int = 0, cache_read_tokens: int = 0, cost_usd: float = 0.0, duration_ms: float = 0.0)</code>  <code>dataclass</code>","text":"<p>Token usage and cost from a single model turn.</p>"},{"location":"reference/result/#pytest_codingagents.copilot.result.SubagentInvocation","title":"<code>SubagentInvocation(name: str, status: str, duration_ms: float | None = None)</code>  <code>dataclass</code>","text":"<p>A subagent invocation observed during execution.</p>"},{"location":"reference/result/#turn-and-toolcall","title":"Turn and ToolCall","text":"<p><code>Turn</code> and <code>ToolCall</code> are re-exported from <code>pytest_aitest.core.result</code> for convenience. See the pytest-aitest documentation for their full API.</p> <pre><code>from pytest_codingagents.copilot.result import Turn, ToolCall\n</code></pre>"}]}